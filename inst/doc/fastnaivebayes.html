<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Martin Skogholt" />

<meta name="date" content="2020-02-09" />

<title>Fast Naive Bayes</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Fast Naive Bayes</h1>
<h4 class="author"><em>Martin Skogholt</em></h4>
<h4 class="date"><em>2020-02-09</em></h4>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This is an extremely fast implementation of a Naive Bayes classifier. This package is currently the only package that supports a Bernoulli distribution, a Multinomial distribution, and a Gaussian distribution, making it suitable for both binary features, frequency counts, and numerical features. Another feature is the support of a mix of different event models. Only numerical variables are allowed, however, categorical variables can be transformed into dummies and used with the Bernoulli distribution.</p>
<p>This implementation offers a huge performance gain compared to other implementations in R. The execution times were compared on a data set of tweets and this package was found to be around 283 to 34,841 times faster for the Bernoulli event models and 17 to 60 times faster for the Multinomial model. For the Gaussian distribution this package was found to be between 2.8 and 1679 times faster. The implementation is largely based on the paper “A comparison of event models for Naive Bayes anti-spam e-mail filtering” written by K.M. Schneider (2003).</p>
<p>Any issues can be submitted to: <a href="https://github.com/mskogholt/fastNaiveBayes/issues" class="uri">https://github.com/mskogholt/fastNaiveBayes/issues</a>.</p>
<p>The purpose of this vignette is to explain some key aspects of this implementation in detail. Firstly, a short introduction to text classification is given as the context for further explanations about the Naive Bayes classifier. It should be noted that the Naive Bayes classifier is not restricted to text classification. The Naive Bayes classifier is a general classification algorithm, but most commonly applied to text classification. Secondly, the general framework of a Naive Bayes classifier is outlined in order to subsequently delve deeper into the different event models. Thirdly, a mathematical explanation is given as to why this particular implementation has such an excellent performance in terms of speed. In the fourth section a description is given about the unique features that sets this implementation of a Naive Bayes classifier apart from other implementations within the R community. Lastly, some code examples are included.</p>
</div>
<div id="text-classification" class="section level2">
<h2>Text Classification</h2>
<p>Text classification is the task of classifying documents by their content: that is, by the words of which they are comprised. The documents are often represented as a bag of words. This means that only the occurrence or frequency of the words in the document are taken into account, any information about the syntactic structure of these words is discarded (Hu &amp; Liu, 2012). In many research efforts regarding document classification, Naive Bayes has been successfully applied (McCallum &amp; Nigam, 1998). Furthermore, text classification will serve as the basis for further elaboration on the inner workings of the Naive Bayes classifier and the different event models.</p>
</div>
<div id="naive-bayes" class="section level2">
<h2>Naive Bayes</h2>
<p>Naive Bayes is a probabilistic classification method based on the Bayes theorem with a strong and naive independence assumption. Naive Bayes assumes independence between all attributes. Despite this so-called “Naive Bayes assumption”, this technique has been proven to be very effective for text classification (McCallum &amp; Nigam, 1998). In the context of text classification, Naive Bayes estimates the posterior probability that a document, consisting out of several words, belongs to a certain class and classifies the document as the class which has the highest posterior probability: <span class="math display">\[P(C=k|D) = \frac{P(D|C=k)*P(C=k)}{P(D)}\]</span> Where <span class="math inline">\(P(C=k|D)\)</span> is the posterior probability that the class equals <span class="math inline">\(k\)</span> given document, <span class="math inline">\(D\)</span>. The Bayes theorem is applied to rewrite this probability to three components:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P(D)\)</span>, the prior probability of document, <span class="math inline">\(D\)</span></li>
<li><span class="math inline">\(P(C=k)\)</span>, the prior probability of class, <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(P(D|C=k)\)</span>, the conditional probability of document, <span class="math inline">\(D\)</span>, given class, <span class="math inline">\(k\)</span></li>
</ol>
<p>To classify a document, <span class="math inline">\(D\)</span>, the class, <span class="math inline">\(k\)</span>, with the highest probability is chosen as the classification. This means that we can simplify the equation a bit, since <span class="math inline">\(P(D)\)</span> is the same for all classes. By removing the denominator, the focus is now solely on calculating the nominator, i.e. the first 2 components.</p>
<div id="the-prior" class="section level3">
<h3>The prior</h3>
<p>The prior probability of class, <span class="math inline">\(k\)</span>, i.e. <span class="math inline">\(P(C=k)\)</span>, is simply the proportion of documents in the training dataset that have class, <span class="math inline">\(k\)</span>. For example, if our training dataset consists of 100 emails that have been labeled as either <span class="math inline">\(Ham\)</span> or <span class="math inline">\(Spam\)</span> and there were 63 emails that were labeled <span class="math inline">\(Ham\)</span> and 37 emails labeled as <span class="math inline">\(Spam\)</span>. In this case, <span class="math inline">\(P(C=Spam)\)</span> is the proportion of emails that were labeled as <span class="math inline">\(Spam\)</span>, i.e. <span class="math inline">\(\frac{37}{100}=0.37\)</span>. This prior probability estimation is the same regardless of which distribution is used within the Naive Bayes Classifier.</p>
</div>
<div id="event-models" class="section level3">
<h3>Event models</h3>
<p>Naive Bayes is a popular classification method, however, within the classification community there is some confusion about this classifier: There are three different generative models in common use, the Multinomial Naive Bayes, Bernoulli Naive Bayes, and finally the Gaussian Naive Bayes. Most confusion is surrounding the Multinomial and Bernoulli event models. Both are called Naive Bayes by their practitioners and both make use of the Naive Bayes assumption. However, they have different assumptions on the distributions of the features that are used. This means that these assumptions lead to two distinct models, which are very often confused (McCallum &amp; Nigam, 1998).</p>
<div id="bernoulli-distribution" class="section level4">
<h4>Bernoulli Distribution</h4>
<p>The most commonly used Naive Bayes classifier uses a Bernoulli model. This is applicable for binary features that indicate the presence or absence of a feature(1 and 0, respectively). Each document, <span class="math inline">\(D\)</span>, consists of a set of words, <span class="math inline">\(w\)</span>. Let <span class="math inline">\(V\)</span> be the vocabulary, i.e. the collection of unique words in the complete dataset. Using the Bernoulli distribution, <span class="math inline">\(P(D_i|C=k)\)</span> becomes: <span class="math display">\[P(D_i|C=k) = \prod\limits_{t=1}^{|V|}{b_{i,t}*P(w_{t}|C=k)+(1-b_{i,t})*(1-P(w_{t}|C=k))}\]</span> Where <span class="math inline">\(b_{i,t}=1\)</span> if the document, <span class="math inline">\(D_i\)</span>, contains the word, <span class="math inline">\(w_t\)</span>, and <span class="math inline">\(0\)</span> otherwise. Furthermore, <span class="math inline">\(|V|\)</span> is the number of unique words in the dataset and <span class="math inline">\(P(w_{t}|C=k)\)</span> is the posterior probability of word, <span class="math inline">\(w_t\)</span> occurring in a document with class, <span class="math inline">\(k\)</span>. This is simply calculated as the proportion of documents of class, <span class="math inline">\(k\)</span>, in which word, <span class="math inline">\(t\)</span>, occurs compared the total number of documents of class, <span class="math inline">\(k\)</span>. In other words: <span class="math display">\[P(w_{t}|C=k)=\frac{\sum_{i=1}^{N}{x_{i,t}*z_{i,k}}}{\sum_{i=1}^{N}{z_{i,k}}}\]</span> Where <span class="math inline">\(x_{i,t}\)</span> equals <span class="math inline">\(1\)</span> if word, <span class="math inline">\(t\)</span>, occurs in document, <span class="math inline">\(i\)</span>, and <span class="math inline">\(0\)</span> otherwise. Furthermore, <span class="math inline">\(z_{i,k}\)</span> equals <span class="math inline">\(1\)</span> if document, <span class="math inline">\(i\)</span>, is labeled as class, <span class="math inline">\(k\)</span>, and <span class="math inline">\(0\)</span> otherwise.</p>
</div>
<div id="multinomial-distribution" class="section level4">
<h4>Multinomial Distribution</h4>
<p>The multinomial distribution is used to model features, which represent the frequency of which the events occurred, or in other words it uses word counts in the documents instead of the binary representation. This means that the distribution used to calculate <span class="math inline">\(P(D_i|C=k)\)</span> changes. This now becomes: <span class="math display">\[P(D_i|C=k) = \prod\limits_{t=1}^{|V|}{P(w_t|C=k)^{x_{i,t}}}\]</span> Where <span class="math inline">\(x_{i,t}\)</span> is the frequency of word, <span class="math inline">\(t\)</span>, in document, <span class="math inline">\(i\)</span>. Here: <span class="math display">\[P(w_t|C=k)=\frac{\sum_{i=1}^{N}{x_{i,t}*z_{i,k}}}{\sum_{s=1}^{|V|}{\sum_{i=1}^{N}{x_{i,s}z_{i,k}}}}\]</span> Where <span class="math inline">\(x_{i,t}\)</span> is the frequency of word, <span class="math inline">\(t\)</span>, in document, <span class="math inline">\(i\)</span> and <span class="math inline">\(z_{i,k}\)</span> equals <span class="math inline">\(1\)</span> if document, <span class="math inline">\(i\)</span>, is labeled as class, <span class="math inline">\(k\)</span>, and <span class="math inline">\(0\)</span> otherwise. Furthermore, <span class="math inline">\(|V|\)</span> is the length of the vocabulary, i.e. the total number of unique words in the dataset.</p>
</div>
<div id="gaussian-distribution" class="section level4">
<h4>Gaussian Distribution</h4>
<p>A Gaussian distribution can also be used to model numerical features. Quite simply the conditional probabilities are now assumed to follow a normal distribution, where the mean and standard deviation are estimated from the training data. In this case, <span class="math inline">\(P(D_i|C=k)\)</span> becomes: <span class="math display">\[P(D_i|C=k) = \prod\limits_{t=1}^{|V|}{P(w_t|C=k)}\]</span> where <span class="math display">\[P(w_t|C=k)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span> where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are estimated by their sample estimators from the training data.</p>
</div>
<div id="mixed-distributions" class="section level4">
<h4>Mixed Distributions</h4>
<p>As was explained, all three event models are part of a general Naive Bayes framework and all three prescribe different ways to estimate <span class="math display">\[P(D_i|C=k)\]</span>. Furthermore, all three use the general Naive Bayes approach, which is to assume independence between the features and simply use the product of each individual probability, as follows: <span class="math display">\[P(D_i|C=k) = \prod\limits_{t=1}^{|V|}{P(w_t|C=k)}\]</span> A big benefit of this independence assumption is that different event models can be mixed simply by using the individual event models for different features.</p>
</div>
</div>
<div id="laplace-smoothing" class="section level3">
<h3>Laplace Smoothing</h3>
<p>Another important aspect of Naive Bayes classifiers is the so-called Laplace smoothing. Consider again the probability calculation: <span class="math display">\[P(D_i|C=k) = \prod\limits_{t=1}^{|V|}{b_{i,t}*P(w_{t}|C=k)+(1-b_{i,t})*(1-P(w_{t}|C=k))}\]</span> If at any point <span class="math inline">\(P(w_t|C=k)=0\)</span>, then <span class="math inline">\(P(D_i|C=k)\)</span> will also equal <span class="math inline">\(0\)</span>, since it’s a product of the individual probabilities. The same holds for the Multinomial distribution. In order to overcome this, Laplace smoothing is used, which simply adds a small non-zero count to all the word counts, so as to not encounter zero probabilities. There is a very important distinction to be made. A commonly made mistake is to assume that this is also applied to any features in the test set that were not encountered in the training set. This however, is not correct. The Laplace smoothing is applied, such that words that do not occur at all together with a specific class do not yield zero probabilities. Features in the test set that were not encountered in the training set are simply ignored from the equation. This also makes sense, if a word was never encountered in the training set then <span class="math inline">\(P(w_t|C=k)\)</span> should be the same for every class, <span class="math inline">\(k\)</span>.</p>
</div>
</div>
<div id="why-is-it-so-fast" class="section level2">
<h2>Why is it so fast?</h2>
<p>As previously explained, when classifying a new document, one needs to calculate <span class="math inline">\(P(C=k|D_i) = \frac{P(D_i|C=k)*P(C=k)}{P(D_i)}\)</span> for each class, <span class="math inline">\(k\)</span>. However, since the class with the highest posterior probability is used as the classification and <span class="math inline">\(P(D_i)\)</span> is constant for all classes, the denominator can be ignored. This means that for prediction, only <span class="math inline">\(P(D_i|C=k)*P(C=k)\)</span> needs to be calculated. As has been shown above this probability in the Bernoulli case can be rewritten to: <span class="math display">\[P(D_i|C=k) = \prod\limits_{t=1}^{|V|}{b_{i,t}*P(w_{t}|C=k)+(1-b_{i,t})*(1-P(w_{t}|C=k))}\]</span> By taking the log transformation this becomes: <span class="math display">\[log(\prod\limits_{t=1}^{|V|}{b_{i,t}*P(w_{t}|C=k)+(1-b_{i,t})*(1-P(w_{t}|C=k))}) = \sum_{t=1}^{|V|}{log(b_{i,t}*P(w_{t}|C=k)+(1-b_{i,t})*(1-P(w_{t}|C=k)))}\]</span> Furthermore, by rearranging some terms this becomes: <span class="math display">\[\sum_{t=1}^{|V|}{b_{i,t}*log(P(w_{t}|C=k))} + \sum_{t=1}^{|V|}{(1-b_{i,t})*log((1-P(w_{t}|C=k)))} \]</span> If we zoom in on the first part and keep in mind that our matrix, <span class="math inline">\(x\)</span>, with observations is a matrix where each column represents a word, from <span class="math inline">\(1\)</span> to <span class="math inline">\(|V|\)</span>, with a <span class="math inline">\(1\)</span> if the word was observed and <span class="math inline">\(0\)</span> otherwise. This means that the matrix of observations has <span class="math inline">\(b_{i,t}\)</span> as the values. The probabilities, <span class="math inline">\(P(w_t|C=k)\)</span>, is a vector of length <span class="math inline">\(|V|\)</span>. We can now use matrix multiplication to derive the sum as follows: <span class="math inline">\(x * P(w_t|C=k)\)</span> for the first part and <span class="math inline">\((1-x) * (1-P(w_t|C=k))\)</span> for the second part. After these two parts have been added up, one can simply raise <span class="math inline">\(e\)</span> to the power of the outcomes to transform it back to the original probabilities. This mathematical trick is what allows one to use matrix multiplication, which in turn is what makes this specific implementation so efficient.</p>
</div>
<div id="unique-features" class="section level2">
<h2>Unique Features</h2>
<p>In this section, a brief overview is given of the unique features of this package. This implementation improves upon existing implementations on two points:</p>
<ol style="list-style-type: decimal">
<li>Speed of execution: by using the matrix multiplication trick this package is magnitudes faster</li>
<li>Easily mix event models by using the mixed model.</li>
<li>The only R package with Bernoulli, Multinomial, and Gaussian event models implemented.</li>
</ol>
<p>In order to demonstrate the power of this package a comparison of estimation and prediction execution times has been done using this package and been compared to different packages. The comparison was made on a dataset consisting of 14640 tweets, where all were used to train the Naive Bayes classifier and all tweets were used to test. After processing a total of 2214 features, i.e. words, were used. In the table below the comparison between execution times is shown. The reported figures are measured in seconds and is the amount of time to train and predict a single time on the tweets data.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Bernoulli</th>
<th>Multinomial</th>
<th>Gaussian</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>fastNaiveBayes</td>
<td>0.263</td>
<td>0.193</td>
<td>0.005</td>
</tr>
<tr class="even">
<td>fastNaiveBayes_sparse</td>
<td>0.015</td>
<td>0.012</td>
<td>0.043</td>
</tr>
<tr class="odd">
<td>bnlearn</td>
<td>5.976</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>e1071</td>
<td>522.618</td>
<td></td>
<td>8.397</td>
</tr>
<tr class="odd">
<td>klar</td>
<td>421.323</td>
<td></td>
<td>8.040</td>
</tr>
<tr class="even">
<td>naivebayes</td>
<td>4.247</td>
<td></td>
<td>0.349</td>
</tr>
<tr class="odd">
<td>quanteda</td>
<td>8.075</td>
<td>0.200</td>
<td></td>
</tr>
<tr class="even">
<td>Rfast</td>
<td></td>
<td>0.724</td>
<td>0.014</td>
</tr>
</tbody>
</table>
<p>For a relative comparison, the figures are also given with the shortest execution time is standardized to 1 in the table below:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Bernoulli</th>
<th>Multinomial</th>
<th>Gaussian</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>fastNaiveBayes</td>
<td>17.5</td>
<td>16.8</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>fastNaiveBayes_sparse</td>
<td>1.0</td>
<td>1.0</td>
<td>8.6</td>
</tr>
<tr class="odd">
<td>bnlearn</td>
<td>398.4</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>e1071</td>
<td>34841.2</td>
<td></td>
<td>1679.4</td>
</tr>
<tr class="odd">
<td>klaR</td>
<td>28088.2</td>
<td></td>
<td>1608</td>
</tr>
<tr class="even">
<td>naivebayes</td>
<td>283.1</td>
<td></td>
<td>69.8</td>
</tr>
<tr class="odd">
<td>quanteda</td>
<td>538.3</td>
<td>16.7</td>
<td></td>
</tr>
<tr class="even">
<td>Rfast</td>
<td></td>
<td>60.3</td>
<td>2.8</td>
</tr>
</tbody>
</table>
<p>As can be seen from the results, this package is magnitudes faster for all event models. Using only a Bernoulli event model, the smallest speed-up was compared to the ‘naivebayes’ package, where this package was found to be 283 times faster. The largerst speed-up was compared to the ‘klaR’ and ‘e1071’ packages, where this package is around 28,088 and 34,841 times faster, respectively. It seems unbelievable, but it should be noted that the data set of tweets resulted in a very sparse matrix. This is why the sparse matrix combined with the matrix multiplication results in such a large increase in speed.</p>
<p>For the Multinomial event model, there’s only two alternative implementations, from the ‘quanteda’ and ‘Rfast’ package. This implementation was found to be 17 times and 60 times faster, respectively.</p>
<p>Lastly, comparing the Gaussian event model the smallest speed-up was compared to the ‘Rfast’ package of 2.8 times. Compared to the ‘naivebayes’ package a speed-up of 70 times was achieved and finally compared to the ‘e1071’ and ‘klaR’ packages this package was found to be 1680 and 1608 times faster, respectively. Using a sparse matrix did not result in a faster execution time. This makes sense since the data used to test the Gaussian distribution is not sparse at all.</p>
<p>It should be noted, that these results can vary a lot between data sets and is dependent on both hardware and software. The tweets data is very sparse when converted to a document-term matrix and hence this is probably a best case scenario. In order to make it easier to compare execution times, the tweets data that was used to establish these results are included in the package as ‘tweets’, the raw data, and ‘tweetsDTM’ a clean document-term matrix of the previously mentioned ‘tweets’ data. The code used to convert the raw ‘tweets’ data can be found on github in the ‘data-raw’ folder. Moreover, the code to establish the results can be found below.</p>
<div id="code-to-compare-execution-times" class="section level3">
<h3>Code to compare Execution Times</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rm</span>(<span class="dt">list=</span><span class="kw">ls</span>())
###################### LIBRARIES ###########################
<span class="kw">library</span>(tm) <span class="co">#used for text mining </span>
<span class="kw">library</span>(e1071) <span class="co">#this package includes the naive Bayes algorithm</span>
<span class="kw">library</span>(Matrix)
<span class="kw">library</span>(microbenchmark)
<span class="kw">library</span>(e1071)
<span class="kw">library</span>(fastNaiveBayes)
<span class="kw">library</span>(quanteda)
<span class="kw">library</span>(naivebayes)
<span class="kw">library</span>(bnlearn)
<span class="kw">library</span>(klaR)
<span class="kw">library</span>(data.table)

############################ Timing Script ################
results &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="co"># Bernoulli Event Model</span>
tweets &lt;-<span class="st"> </span>fastNaiveBayes<span class="op">::</span>tweetsDTM


y_var &lt;-<span class="st"> </span>tweets<span class="op">$</span>airline_sentiment
y_var &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(y_var<span class="op">==</span><span class="st">'negative'</span>,<span class="st">'negative'</span>,<span class="st">'non-negative'</span>))
tweets &lt;-<span class="st"> </span>tweets[,<span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(tweets)]
tweets[tweets<span class="op">&gt;</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="dv">1</span>

tweets &lt;-<span class="st"> </span>tweets[,<span class="kw">which</span>(<span class="kw">colSums</span>(tweets)<span class="op">!=</span><span class="dv">0</span>)]
tweets &lt;-<span class="st"> </span>tweets[,<span class="kw">which</span>(<span class="kw">colSums</span>(tweets)<span class="op">!=</span><span class="kw">nrow</span>(tweets))]

tweet_mat &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(tweets)
sparse_tweets &lt;-<span class="st"> </span><span class="kw">Matrix</span>(<span class="kw">as.matrix</span>(tweet_mat), <span class="dt">sparse =</span> <span class="ot">TRUE</span>)

<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(tweets)){
  tweets[[i]] &lt;-<span class="st"> </span><span class="kw">as.factor</span>(tweets[[i]])
}

<span class="co"># BNLearn</span>
bn_tweets &lt;-<span class="st"> </span><span class="kw">cbind</span>(y_var, tweets)
<span class="kw">colnames</span>(bn_tweets)[<span class="dv">1</span>] &lt;-<span class="st"> 'y_var'</span>

<span class="co"># Quanteda</span>
dfm &lt;-<span class="st"> </span><span class="kw">as.dfm</span>(tweet_mat)

res &lt;-<span class="st"> </span><span class="kw">microbenchmark</span>(
  <span class="dt">klar =</span> <span class="kw">predict</span>(klaR<span class="op">::</span><span class="kw">NaiveBayes</span>(<span class="dt">x=</span>tweets, <span class="dt">grouping =</span> y_var, <span class="dt">fL=</span><span class="dv">1</span>), tweets),
  <span class="dt">e1071 =</span> <span class="kw">predict</span>(e1071<span class="op">::</span><span class="kw">naiveBayes</span>(tweets, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), tweets),
  <span class="dt">fastNaiveBayes =</span> <span class="kw">predict</span>(<span class="kw">fastNaiveBayes.bernoulli</span>(tweet_mat, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), tweet_mat),
  <span class="dt">fastNaiveBayes_sparse =</span> <span class="kw">predict</span>(<span class="kw">fastNaiveBayes.bernoulli</span>(sparse_tweets, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), sparse_tweets),
  <span class="dt">bnlearn =</span> <span class="kw">predict</span>(bnlearn<span class="op">::</span><span class="kw">naive.bayes</span>(bn_tweets, <span class="st">'y_var'</span>), bn_tweets),
  <span class="dt">quanteda =</span> <span class="kw">predict</span>(quanteda<span class="op">::</span><span class="kw">textmodel_nb</span>(dfm, y_var, <span class="dt">prior =</span> <span class="st">&quot;docfreq&quot;</span>, <span class="dt">distribution =</span> <span class="st">&quot;Bernoulli&quot;</span>),
                     <span class="dt">newdata =</span> dfm),
  <span class="dt">naivebayes =</span> <span class="kw">predict</span>(naivebayes<span class="op">::</span><span class="kw">naive_bayes</span>(tweets, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), <span class="dt">newdata =</span> tweets),
  <span class="dt">times =</span> <span class="dv">3</span>,
  <span class="dt">unit =</span> <span class="st">&quot;ms&quot;</span>
)

res &lt;-<span class="st"> </span><span class="kw">as.data.table</span>(res)
res[,nrows<span class="op">:</span><span class="er">=</span><span class="kw">nrow</span>(tweet_mat)]
res[,ncols<span class="op">:</span><span class="er">=</span><span class="kw">ncol</span>(tweet_mat)]
res[,model<span class="op">:</span><span class="er">=</span><span class="st">'Bernoulli'</span>]

results &lt;-<span class="st"> </span>res

<span class="co"># Multinomial Event Model</span>
tweets &lt;-<span class="st"> </span>fastNaiveBayes<span class="op">::</span>tweetsDTM

y_var &lt;-<span class="st"> </span>tweets<span class="op">$</span>airline_sentiment
y_var &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(y_var<span class="op">==</span><span class="st">'negative'</span>,<span class="st">'negative'</span>,<span class="st">'non-negative'</span>))
tweets &lt;-<span class="st"> </span>tweets[,<span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(tweets)]

tweets &lt;-<span class="st"> </span>tweets[,<span class="kw">which</span>(<span class="kw">colSums</span>(tweets)<span class="op">!=</span><span class="dv">0</span>)]

tweet_mat &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(tweets)
sparse_tweets &lt;-<span class="st"> </span><span class="kw">Matrix</span>(<span class="kw">as.matrix</span>(tweet_mat), <span class="dt">sparse =</span> <span class="ot">TRUE</span>)

<span class="co"># Quanteda</span>
dfm &lt;-<span class="st"> </span><span class="kw">as.dfm</span>(tweet_mat)

res &lt;-<span class="st"> </span><span class="kw">microbenchmark</span>(
  <span class="dt">fastNaiveBayes =</span> <span class="kw">predict</span>(<span class="kw">fastNaiveBayes.multinomial</span>(tweet_mat, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), tweet_mat),
  <span class="dt">fastNaiveBayes_sparse =</span> <span class="kw">predict</span>(<span class="kw">fastNaiveBayes.multinomial</span>(sparse_tweets, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), sparse_tweets),
  <span class="dt">quanteda =</span> <span class="kw">predict</span>(quanteda<span class="op">::</span><span class="kw">textmodel_nb</span>(dfm, y_var, <span class="dt">prior =</span> <span class="st">&quot;docfreq&quot;</span>, <span class="dt">distribution =</span> <span class="st">&quot;multinomial&quot;</span>),
                     <span class="dt">newdata =</span> dfm),
  <span class="dt">Rfast =</span> Rfast<span class="op">::</span><span class="kw">multinom.nb</span>(tweet_mat, tweet_mat, y_var),
  <span class="dt">times =</span> <span class="dv">3</span>,
  <span class="dt">unit =</span> <span class="st">&quot;ms&quot;</span>
)

res &lt;-<span class="st"> </span><span class="kw">as.data.table</span>(res)
res[,nrows<span class="op">:</span><span class="er">=</span><span class="kw">nrow</span>(tweet_mat)]
res[,ncols<span class="op">:</span><span class="er">=</span><span class="kw">ncol</span>(tweet_mat)]
res[,model<span class="op">:</span><span class="er">=</span><span class="st">'Multinomial'</span>]

results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, res)

<span class="co"># Gaussian Event Model</span>
cars &lt;-<span class="st"> </span>mtcars
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>){
  cars &lt;-<span class="st"> </span><span class="kw">rbind</span>(cars, cars)
}

y_var &lt;-<span class="st"> </span>cars<span class="op">$</span>mpg
y_var &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(y_var<span class="op">&gt;</span><span class="dv">20</span>,<span class="st">'negative'</span>,<span class="st">'non-negative'</span>))

cars &lt;-<span class="st"> </span>cars[,<span class="dv">3</span><span class="op">:</span><span class="dv">7</span>]
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>){
  cars &lt;-<span class="st"> </span><span class="kw">cbind</span>(cars, cars)
}

cars_mat &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(cars)
sparse_cars &lt;-<span class="st"> </span><span class="kw">Matrix</span>(<span class="kw">as.matrix</span>(cars_mat), <span class="dt">sparse =</span> <span class="ot">TRUE</span>)

res &lt;-<span class="st"> </span><span class="kw">microbenchmark</span>(
  <span class="dt">klar =</span> <span class="kw">predict</span>(klaR<span class="op">::</span><span class="kw">NaiveBayes</span>(<span class="dt">x=</span>cars_mat, <span class="dt">grouping =</span> y_var, <span class="dt">fL=</span><span class="dv">1</span>), cars_mat),
  <span class="dt">e1071 =</span> <span class="kw">predict</span>(e1071<span class="op">::</span><span class="kw">naiveBayes</span>(cars_mat, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), cars_mat),
  <span class="dt">naivebayes =</span> <span class="kw">predict</span>(naivebayes<span class="op">::</span><span class="kw">naive_bayes</span>(cars_mat, y_var, <span class="dt">laplace =</span> <span class="dv">1</span>), <span class="dt">newdata =</span> cars_mat),
  <span class="dt">fastNaiveBayes =</span> <span class="kw">predict</span>(<span class="kw">fastNaiveBayes.gaussian</span>(cars_mat, y_var), cars_mat),
  <span class="dt">fastNaiveBayes_sparse =</span> <span class="kw">predict</span>(<span class="kw">fastNaiveBayes.gaussian</span>(sparse_cars, y_var), sparse_cars),
  <span class="dt">Rfast =</span> Rfast<span class="op">::</span><span class="kw">gaussian.nb</span>(cars_mat, cars_mat, y_var),
  <span class="dt">times =</span> <span class="dv">3</span>,
  <span class="dt">unit =</span> <span class="st">&quot;ms&quot;</span>
)

res &lt;-<span class="st"> </span><span class="kw">as.data.table</span>(res)
res[,nrows<span class="op">:</span><span class="er">=</span><span class="kw">nrow</span>(cars_mat)]
res[,ncols<span class="op">:</span><span class="er">=</span><span class="kw">ncol</span>(cars_mat)]
res[,model<span class="op">:</span><span class="er">=</span><span class="st">'Gaussian'</span>]

results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, res)

<span class="kw">print</span>(results)
<span class="kw">fwrite</span>(results, <span class="dt">file =</span> <span class="st">&quot;./package_timings.csv&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</code></pre></div>
</div>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rm</span>(<span class="dt">list=</span><span class="kw">ls</span>())
<span class="kw">library</span>(fastNaiveBayes)

cars &lt;-<span class="st"> </span>mtcars
y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">ifelse</span>(cars<span class="op">$</span>mpg<span class="op">&gt;</span><span class="dv">25</span>,<span class="st">'High'</span>,<span class="st">'Low'</span>))
x &lt;-<span class="st"> </span>cars[,<span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(cars)]

<span class="co"># Mixed event models</span>
dist &lt;-<span class="st"> </span>fastNaiveBayes<span class="op">::</span><span class="kw">fastNaiveBayes.detect_distribution</span>(x, <span class="dt">nrows =</span> <span class="kw">nrow</span>(x))
<span class="kw">print</span>(dist)
mod &lt;-<span class="st"> </span><span class="kw">fastNaiveBayes.mixed</span>(x,y,<span class="dt">laplace =</span> <span class="dv">1</span>)
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> x)
<span class="kw">mean</span>(pred<span class="op">!=</span>y)

<span class="co"># Bernoulli only</span>
vars &lt;-<span class="st"> </span><span class="kw">c</span>(dist<span class="op">$</span>bernoulli, dist<span class="op">$</span>multinomial)
newx &lt;-<span class="st"> </span>x[,vars]
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(newx)){
 newx[[i]] &lt;-<span class="st"> </span><span class="kw">as.factor</span>(newx[[i]])
}
new_mat &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(y <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="dv">1</span>, <span class="kw">cbind</span>(y,newx))
mod &lt;-<span class="st"> </span><span class="kw">fastNaiveBayes.bernoulli</span>(new_mat, y, <span class="dt">laplace =</span> <span class="dv">1</span>)
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> new_mat)
<span class="kw">mean</span>(pred<span class="op">!=</span>y)

<span class="co"># Construction sparse Matrix:</span>
mod &lt;-<span class="st"> </span><span class="kw">fastNaiveBayes.bernoulli</span>(new_mat, y, <span class="dt">laplace =</span> <span class="dv">1</span>, <span class="dt">sparse =</span> <span class="ot">TRUE</span>)
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> new_mat)
<span class="kw">mean</span>(pred<span class="op">!=</span>y)

<span class="co"># OR:</span>
new_mat &lt;-<span class="st"> </span>Matrix<span class="op">::</span><span class="kw">Matrix</span>(<span class="kw">as.matrix</span>(new_mat), <span class="dt">sparse =</span> <span class="ot">TRUE</span>)
mod &lt;-<span class="st"> </span><span class="kw">fastNaiveBayes.bernoulli</span>(new_mat, y, <span class="dt">laplace =</span> <span class="dv">1</span>)
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> new_mat)
<span class="kw">mean</span>(pred<span class="op">!=</span>y)

<span class="co"># Multinomial only</span>
vars &lt;-<span class="st"> </span><span class="kw">c</span>(dist<span class="op">$</span>bernoulli, dist<span class="op">$</span>multinomial)
newx &lt;-<span class="st"> </span>x[,vars]
mod &lt;-<span class="st"> </span><span class="kw">fastNaiveBayes.multinomial</span>(newx, y, <span class="dt">laplace =</span> <span class="dv">1</span>)
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> newx)
<span class="kw">mean</span>(pred<span class="op">!=</span>y)

<span class="co"># Gaussian only</span>
vars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">'hp'</span>, dist<span class="op">$</span>gaussian)
newx &lt;-<span class="st"> </span>x[,vars]
mod &lt;-<span class="st"> </span><span class="kw">fastNaiveBayes.gaussian</span>(newx, y)
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> newx)
<span class="kw">mean</span>(pred<span class="op">!=</span>y)</code></pre></div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Hu, X., &amp; Liu, H. (2012). Text analytics in social media. In Mining text data (pp. 385-414). Springer, Boston, MA.</p>
<p>McCallum, A., &amp; Nigam, K. (1998, July). A comparison of event models for naive bayes text classification. In AAAI-98 workshop on learning for text categorization (Vol. 752, No. 1, pp. 41-48).</p>
<p>Schneider, K. M. (2003, April). A comparison of event models for Naive Bayes anti-spam e-mail filtering. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1 (pp. 307-314). Association for Computational Linguistics.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
